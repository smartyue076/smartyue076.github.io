<!DOCTYPE html>
<html lang=zh>
<head>
    <!-- so meta -->
    <meta charset="utf-8">
    <meta http-equiv="X-UA-Compatible" content="IE=edge">
    <meta name="HandheldFriendly" content="True">
    <meta name="viewport" content="width=device-width, initial-scale=1, maximum-scale=5" />
    <meta name="description" content="1. 选择合适的模型对于回归或分类问题使用合适的模型，在本文中对于回归问题使用线形回归模型，对于分类问题使用Logistic回归模型 2. 如何判定当前模型及参数是符合现实要求的（Cost Function）$$J(w,b) &#x3D; \frac{1}{2m} \sum\limits_{i &#x3D; 0}^{m-1} (f_{w,b}(x^{(i)}) - y^{(i)})^2$$ $$f">
<meta property="og:type" content="article">
<meta property="og:title" content="机器学习基础（一）基础方法与概念（线性回归为例）">
<meta property="og:url" content="https://smartyue076.github.io/2025/01/21/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0%E5%9F%BA%E7%A1%80/index.html">
<meta property="og:site_name" content="Martin XU的技术博客">
<meta property="og:description" content="1. 选择合适的模型对于回归或分类问题使用合适的模型，在本文中对于回归问题使用线形回归模型，对于分类问题使用Logistic回归模型 2. 如何判定当前模型及参数是符合现实要求的（Cost Function）$$J(w,b) &#x3D; \frac{1}{2m} \sum\limits_{i &#x3D; 0}^{m-1} (f_{w,b}(x^{(i)}) - y^{(i)})^2$$ $$f">
<meta property="og:locale">
<meta property="og:image" content="https://smartyue076.github.io/images/gradient_algo_1.png">
<meta property="og:image" content="https://smartyue076.github.io/images/gradient_algo_2.png">
<meta property="og:image" content="https://smartyue076.github.io/images/gradient_algo_3.png">
<meta property="og:image" content="https://smartyue076.github.io/images/C1_W3_LinearCostRegularized.png">
<meta property="og:image" content="https://smartyue076.github.io/images/C1_W3_LinearGradientRegularized.png">
<meta property="og:image" content="https://smartyue076.github.io/images/C1_W3_LogisticCostRegularized.png">
<meta property="og:image" content="https://smartyue076.github.io/images/C1_W3_LogisticGradientRegularized.png">
<meta property="article:published_time" content="2025-01-21T10:49:32.076Z">
<meta property="article:modified_time" content="2025-01-21T10:49:32.076Z">
<meta property="article:author" content="Martin XU">
<meta property="article:tag" content="机器学习">
<meta name="twitter:card" content="summary">
<meta name="twitter:image" content="https://smartyue076.github.io/images/gradient_algo_1.png">
    
    
      
        
          <link rel="shortcut icon" href="/images/favicon.ico">
        
      
      
        
          <link rel="icon" type="image/png" href="/images/favicon-192x192.png" sizes="192x192">
        
      
      
        
          <link rel="apple-touch-icon" sizes="180x180" href="/images/apple-touch-icon.png">
        
      
    
    <!-- title -->
    <title>机器学习基础（一）基础方法与概念（线性回归为例）</title>
    <!-- async scripts -->
    <!-- Google Analytics -->

  <script async src="https://www.googletagmanager.com/gtag/js?id=UA-86660611-1"></script>
  <script>
      window.dataLayer = window.dataLayer || [];
      function gtag(){dataLayer.push(arguments);}
      gtag('js', new Date());
      gtag('config', 'UA-86660611-1');
  </script>


    <!-- Umami Analytics -->


    <!-- styles -->
    
<link rel="stylesheet" href="/css/style.css">

    <!-- persian styles -->
    
    <!-- rss -->
    
    
	<!-- mathjax -->
	
<meta name="generator" content="Hexo 7.3.0"></head>

<body class="max-width mx-auto px3 ltr">
    
      <div id="header-post">
  <a id="menu-icon" href="#" aria-label="Menu"><i class="fa-solid fa-bars fa-lg"></i></a>
  <a id="menu-icon-tablet" href="#" aria-label="Menu"><i class="fa-solid fa-bars fa-lg"></i></a>
  <a id="top-icon-tablet" href="#" aria-label="Top" onclick="$('html, body').animate({ scrollTop: 0 }, 'fast');" style="display:none;"><i class="fa-solid fa-chevron-up fa-lg"></i></a>
  <span id="menu">
    <span id="nav">
      <ul>
        <!--
       --><li><a href="/">Home</a></li><!--
     --><!--
       --><li><a href="/about/">About</a></li><!--
     --><!--
       --><li><a href="/archives/">Writing</a></li><!--
     --><!--
       --><li><a href="/tags/">Tag</a></li><!--
     --><!--
       --><li><a href="/search/">Search</a></li><!--
     --><!--
       --><li><a target="_blank" rel="noopener" href="https://github.com/smartyue076">Projects</a></li><!--
     -->
      </ul>
    </span>
    <br/>
    <span id="actions">
      <ul>
        
        
        <li><a class="icon" aria-label="Next post" href="/2025/01/21/%E5%9B%BE%E7%AE%97%E6%B3%95%EF%BC%88DFS%E4%B8%8EBFS%EF%BC%89/"><i class="fa-solid fa-chevron-right" aria-hidden="true" onmouseover="$('#i-next').toggle();" onmouseout="$('#i-next').toggle();"></i></a></li>
        
        <li><a class="icon" aria-label="Back to top" href="#" onclick="$('html, body').animate({ scrollTop: 0 }, 'fast');"><i class="fa-solid fa-chevron-up" aria-hidden="true" onmouseover="$('#i-top').toggle();" onmouseout="$('#i-top').toggle();"></i></a></li>
        <li><a class="icon" aria-label="Share post" href="#"><i class="fa-solid fa-share-alt" aria-hidden="true" onmouseover="$('#i-share').toggle();" onmouseout="$('#i-share').toggle();" onclick="$('#share').toggle();return false;"></i></a></li>
      </ul>
      <span id="i-prev" class="info" style="display:none;">Previous post</span>
      <span id="i-next" class="info" style="display:none;">Next post</span>
      <span id="i-top" class="info" style="display:none;">Back to top</span>
      <span id="i-share" class="info" style="display:none;">Share post</span>
    </span>
    <br/>
    <div id="share" style="display: none">
      <ul>
  <li><a class="icon" target="_blank" rel="noopener" href="http://www.facebook.com/sharer.php?u=https://smartyue076.github.io/2025/01/21/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0%E5%9F%BA%E7%A1%80/"><i class="fab fa-facebook " aria-hidden="true"></i></a></li>
  <li><a class="icon" target="_blank" rel="noopener" href="https://twitter.com/share?url=https://smartyue076.github.io/2025/01/21/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0%E5%9F%BA%E7%A1%80/&text=机器学习基础（一）基础方法与概念（线性回归为例）"><i class="fab fa-twitter " aria-hidden="true"></i></a></li>
  <li><a class="icon" target="_blank" rel="noopener" href="http://www.linkedin.com/shareArticle?url=https://smartyue076.github.io/2025/01/21/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0%E5%9F%BA%E7%A1%80/&title=机器学习基础（一）基础方法与概念（线性回归为例）"><i class="fab fa-linkedin " aria-hidden="true"></i></a></li>
  <li><a class="icon" target="_blank" rel="noopener" href="https://pinterest.com/pin/create/bookmarklet/?url=https://smartyue076.github.io/2025/01/21/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0%E5%9F%BA%E7%A1%80/&is_video=false&description=机器学习基础（一）基础方法与概念（线性回归为例）"><i class="fab fa-pinterest " aria-hidden="true"></i></a></li>
  <li><a class="icon" href="mailto:?subject=机器学习基础（一）基础方法与概念（线性回归为例）&body=Check out this article: https://smartyue076.github.io/2025/01/21/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0%E5%9F%BA%E7%A1%80/"><i class="fa-solid fa-envelope " aria-hidden="true"></i></a></li>
  <li><a class="icon" target="_blank" rel="noopener" href="https://getpocket.com/save?url=https://smartyue076.github.io/2025/01/21/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0%E5%9F%BA%E7%A1%80/&title=机器学习基础（一）基础方法与概念（线性回归为例）"><i class="fab fa-get-pocket " aria-hidden="true"></i></a></li>
  <li><a class="icon" target="_blank" rel="noopener" href="http://reddit.com/submit?url=https://smartyue076.github.io/2025/01/21/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0%E5%9F%BA%E7%A1%80/&title=机器学习基础（一）基础方法与概念（线性回归为例）"><i class="fab fa-reddit " aria-hidden="true"></i></a></li>
  <li><a class="icon" target="_blank" rel="noopener" href="http://www.stumbleupon.com/submit?url=https://smartyue076.github.io/2025/01/21/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0%E5%9F%BA%E7%A1%80/&title=机器学习基础（一）基础方法与概念（线性回归为例）"><i class="fab fa-stumbleupon " aria-hidden="true"></i></a></li>
  <li><a class="icon" target="_blank" rel="noopener" href="http://digg.com/submit?url=https://smartyue076.github.io/2025/01/21/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0%E5%9F%BA%E7%A1%80/&title=机器学习基础（一）基础方法与概念（线性回归为例）"><i class="fab fa-digg " aria-hidden="true"></i></a></li>
  <li><a class="icon" target="_blank" rel="noopener" href="http://www.tumblr.com/share/link?url=https://smartyue076.github.io/2025/01/21/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0%E5%9F%BA%E7%A1%80/&name=机器学习基础（一）基础方法与概念（线性回归为例）&description="><i class="fab fa-tumblr " aria-hidden="true"></i></a></li>
  <li><a class="icon" target="_blank" rel="noopener" href="https://news.ycombinator.com/submitlink?u=https://smartyue076.github.io/2025/01/21/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0%E5%9F%BA%E7%A1%80/&t=机器学习基础（一）基础方法与概念（线性回归为例）"><i class="fab fa-hacker-news " aria-hidden="true"></i></a></li>
</ul>

    </div>
    
    
      <div id="toc">
        <ol class="toc"><li class="toc-item toc-level-2"><a class="toc-link" href="#1-%E9%80%89%E6%8B%A9%E5%90%88%E9%80%82%E7%9A%84%E6%A8%A1%E5%9E%8B"><span class="toc-number">1.</span> <span class="toc-text">1. 选择合适的模型</span></a></li><li class="toc-item toc-level-2"><a class="toc-link" href="#2-%E5%A6%82%E4%BD%95%E5%88%A4%E5%AE%9A%E5%BD%93%E5%89%8D%E6%A8%A1%E5%9E%8B%E5%8F%8A%E5%8F%82%E6%95%B0%E6%98%AF%E7%AC%A6%E5%90%88%E7%8E%B0%E5%AE%9E%E8%A6%81%E6%B1%82%E7%9A%84%EF%BC%88Cost-Function%EF%BC%89"><span class="toc-number">2.</span> <span class="toc-text">2. 如何判定当前模型及参数是符合现实要求的（Cost Function）</span></a></li><li class="toc-item toc-level-2"><a class="toc-link" href="#3-%E6%95%B0%E6%8D%AE%E9%A2%84%E5%A4%84%E7%90%86"><span class="toc-number">3.</span> <span class="toc-text">3. 数据预处理</span></a><ol class="toc-child"><li class="toc-item toc-level-4"><a class="toc-link" href="#%E7%89%B9%E5%BE%81%E7%BC%A9%E6%94%BE%EF%BC%88Feature-Scaling%EF%BC%89"><span class="toc-number">3.0.1.</span> <span class="toc-text">特征缩放（Feature Scaling）</span></a></li></ol></li></ol></li><li class="toc-item toc-level-2"><a class="toc-link" href="#4-%E5%A6%82%E4%BD%95%E8%B0%83%E6%95%B4%E6%A8%A1%E5%9E%8B%E5%8F%82%E6%95%B0"><span class="toc-number">4.</span> <span class="toc-text">4. 如何调整模型参数</span></a><ol class="toc-child"><li class="toc-item toc-level-3"><a class="toc-link" href="#%E5%AD%A6%E4%B9%A0%E6%96%B9%E6%B3%95%EF%BC%88%E6%A2%AF%E5%BA%A6%E4%B8%8B%E9%99%8D%E5%AD%A6%E4%B9%A0%E6%B3%95%EF%BC%88Gradient-Descent%EF%BC%89%EF%BC%89"><span class="toc-number">4.1.</span> <span class="toc-text">学习方法（梯度下降学习法（Gradient Descent））</span></a><ol class="toc-child"><li class="toc-item toc-level-4"><a class="toc-link" href="#%E6%A6%82%E5%BF%B5"><span class="toc-number">4.1.1.</span> <span class="toc-text">概念</span></a></li><li class="toc-item toc-level-4"><a class="toc-link" href="#%E7%9B%B4%E8%A7%82%E8%A1%A8%E7%A4%BA"><span class="toc-number">4.1.2.</span> <span class="toc-text">直观表示</span></a></li><li class="toc-item toc-level-4"><a class="toc-link" href="#%E5%85%AC%E5%BC%8F%E6%8E%A8%E5%AF%BC"><span class="toc-number">4.1.3.</span> <span class="toc-text">公式推导</span></a></li><li class="toc-item toc-level-4"><a class="toc-link" href="#%E5%A6%82%E4%BD%95%E8%AE%BE%E7%BD%AE%E5%90%88%E9%80%82%E7%9A%84%E5%AD%A6%E4%B9%A0%E7%8E%87%EF%BC%88Learning-Rate%EF%BC%89"><span class="toc-number">4.1.4.</span> <span class="toc-text">如何设置合适的学习率（Learning Rate）</span></a></li></ol></li></ol></li><li class="toc-item toc-level-2"><a class="toc-link" href="#5-%E4%BB%80%E4%B9%88%E6%97%B6%E5%80%99%E7%AE%97%E8%AE%AD%E7%BB%83%E5%AE%8C%E6%88%90"><span class="toc-number">5.</span> <span class="toc-text">5. 什么时候算训练完成</span></a></li><li class="toc-item toc-level-2"><a class="toc-link" href="#6-%E5%9C%A8%E5%AD%A6%E4%B9%A0%E8%BF%87%E7%A8%8B%E4%B8%AD%E5%A6%82%E4%BD%95%E5%87%8F%E5%B0%91%E6%AC%A0%E6%8B%9F%E5%90%88%E6%88%96%E8%BF%87%E6%8B%9F%E5%90%88"><span class="toc-number">6.</span> <span class="toc-text">6. 在学习过程中如何减少欠拟合或过拟合</span></a><ol class="toc-child"><li class="toc-item toc-level-3"><a class="toc-link" href="#%E6%AC%A0%E6%8B%9F%E5%90%88%EF%BC%88Underfitting%EF%BC%89"><span class="toc-number">6.1.</span> <span class="toc-text">欠拟合（Underfitting）</span></a><ol class="toc-child"><li class="toc-item toc-level-4"><a class="toc-link" href="#%E7%8E%B0%E8%B1%A1"><span class="toc-number">6.1.1.</span> <span class="toc-text">现象</span></a></li><li class="toc-item toc-level-4"><a class="toc-link" href="#%E8%A7%A3%E5%86%B3%E6%96%B9%E6%B3%95"><span class="toc-number">6.1.2.</span> <span class="toc-text">解决方法</span></a></li></ol></li><li class="toc-item toc-level-3"><a class="toc-link" href="#%E8%BF%87%E6%8B%9F%E5%90%88%EF%BC%88Overfitting%EF%BC%89"><span class="toc-number">6.2.</span> <span class="toc-text">过拟合（Overfitting）</span></a><ol class="toc-child"><li class="toc-item toc-level-4"><a class="toc-link" href="#%E7%8E%B0%E8%B1%A1-1"><span class="toc-number">6.2.1.</span> <span class="toc-text">现象</span></a></li><li class="toc-item toc-level-4"><a class="toc-link" href="#%E8%A7%A3%E5%86%B3%E6%96%B9%E6%B3%95-1"><span class="toc-number">6.2.2.</span> <span class="toc-text">解决方法</span></a></li><li class="toc-item toc-level-4"><a class="toc-link" href="#%E6%AD%A3%E5%88%99%E5%8C%96%EF%BC%88Regularization%EF%BC%89"><span class="toc-number">6.2.3.</span> <span class="toc-text">正则化（Regularization）</span></a><ol class="toc-child"><li class="toc-item toc-level-5"><a class="toc-link" href="#%E7%BA%BF%E6%80%A7%E5%9B%9E%E5%BD%92"><span class="toc-number">6.2.3.1.</span> <span class="toc-text">线性回归</span></a></li><li class="toc-item toc-level-5"><a class="toc-link" href="#Logistic%E5%9B%9E%E5%BD%92"><span class="toc-number">6.2.3.2.</span> <span class="toc-text">Logistic回归</span></a></li></ol></li></ol></li></ol></li><li class="toc-item toc-level-2"><a class="toc-link" href="#0-%E6%95%B0%E6%8D%AE%E5%88%92%E5%88%86%EF%BC%88%E8%AE%AD%E7%BB%83%E9%9B%86-%E9%AA%8C%E8%AF%81%E9%9B%86-%E6%B5%8B%E8%AF%95%E9%9B%86%EF%BC%89"><span class="toc-number">7.</span> <span class="toc-text">0. 数据划分（训练集&#x2F;验证集&#x2F;测试集）</span></a></li></ol>
      </div>
    
  </span>
</div>

    
    <div class="content index py4 ">
        
        <article class="post h-entry" itemscope itemtype="http://schema.org/BlogPosting">
  <header>
    
    <h1 class="posttitle p-name" itemprop="name headline">
        机器学习基础（一）基础方法与概念（线性回归为例）
    </h1>



    <div class="meta">
      <span class="author p-author h-card" itemprop="author" itemscope itemtype="http://schema.org/Person">
        <span class="p-name" itemprop="name">Martin XU</span>
      </span>
      
    <div class="postdate">
      
        <time datetime="2025-01-21T10:49:32.076Z" class="dt-published" itemprop="datePublished">2025-01-21</time>
        
      
    </div>


      

      
    <div class="article-tag">
        <i class="fa-solid fa-tag"></i>
        <a class="p-category" href="/tags/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0/" rel="tag">机器学习</a>
    </div>


    </div>
  </header>
  

  <div class="content e-content" itemprop="articleBody">
    <h2 id="1-选择合适的模型"><a href="#1-选择合适的模型" class="headerlink" title="1. 选择合适的模型"></a>1. 选择合适的模型</h2><p>对于回归或分类问题使用合适的模型，在本文中对于回归问题使用<strong>线形回归</strong>模型，对于分类问题使用<strong>Logistic回归</strong>模型</p>
<h2 id="2-如何判定当前模型及参数是符合现实要求的（Cost-Function）"><a href="#2-如何判定当前模型及参数是符合现实要求的（Cost-Function）" class="headerlink" title="2. 如何判定当前模型及参数是符合现实要求的（Cost Function）"></a>2. 如何判定当前模型及参数是符合现实要求的（Cost Function）</h2><p>$$<br>J(w,b) &#x3D; \frac{1}{2m} \sum\limits_{i &#x3D; 0}^{m-1} (f_{w,b}(x^{(i)}) - y^{(i)})^2<br>$$</p>
<p>$$<br>f_{w,b}(x^{(i)}) &#x3D; wx^{(i)} + b \tag{2}<br>$$</p>
<p>计算预测值与现实值的均方误差后取平均（除以2是为了在求导时与平方的2约去）</p>
<h2 id="3-数据预处理"><a href="#3-数据预处理" class="headerlink" title="3. 数据预处理"></a>3. 数据预处理</h2><h4 id="特征缩放（Feature-Scaling）"><a href="#特征缩放（Feature-Scaling）" class="headerlink" title="特征缩放（Feature Scaling）"></a>特征缩放（Feature Scaling）</h4><p>如果不同属性的取值返回相差过大则会导致模型收敛得很慢，所以要对属性值做映射。</p>
<p>常用缩放方法：<br><strong>均值标准化</strong><br>$$<br>x_i :&#x3D; \dfrac{x_i - \mu_i}{max - min}\tag{Mean normalization}<br>$$</p>
<p><strong>z-score标准化（正态分布）</strong><br>$$<br>x^{(i)}_j &#x3D; \dfrac{x^{(i)}_j - \mu_j}{\sigma_j} \tag{z-score normalization}<br>$$</p>
<h2 id="4-如何调整模型参数"><a href="#4-如何调整模型参数" class="headerlink" title="4. 如何调整模型参数"></a>4. 如何调整模型参数</h2><h3 id="学习方法（梯度下降学习法（Gradient-Descent））"><a href="#学习方法（梯度下降学习法（Gradient-Descent））" class="headerlink" title="学习方法（梯度下降学习法（Gradient Descent））"></a>学习方法（梯度下降学习法（Gradient Descent））</h3><h4 id="概念"><a href="#概念" class="headerlink" title="概念"></a>概念</h4><p>梯度下降是一种优化算法，用于<strong>最小化目标函数（如损失函数）并找到函数的最优参数</strong>。它是机器学习和深度学习中训练模型的核心方法。将<strong>参数更新为原值 ➖偏移量【学习率Learning Rate✖️损失函数对该参数的偏导数】</strong>，直到偏导数为0</p>
<img src="/../../images/gradient_algo_1.png"  style="zoom:25%;" />

<h4 id="直观表示"><a href="#直观表示" class="headerlink" title="直观表示"></a>直观表示</h4><p><strong>目标是使代价函数J(w)最小，即图中函数的最低点</strong>。偏导数在几何上<strong>为在平面上点的切线</strong></p>
<img src="/../../images/gradient_algo_2.png"  style="zoom:25%;" />

<h4 id="公式推导"><a href="#公式推导" class="headerlink" title="公式推导"></a>公式推导</h4><img src="/../../images/gradient_algo_3.png"  style="zoom:25%;" />

<h4 id="如何设置合适的学习率（Learning-Rate）"><a href="#如何设置合适的学习率（Learning-Rate）" class="headerlink" title="如何设置合适的学习率（Learning Rate）"></a>如何设置合适的学习率（Learning Rate）</h4><p>梯度下降算法的每次迭代受到学习率的影响，如果学习率过小，则达到收敛（converge）所需的迭代次数会非常高；如果学习率过大，每次迭代可能不会减小代价函数，可能会越过局部最小值导致无法收敛。可尝试（0.01,0.03,0.1,0.3,1,3,10）</p>
<h2 id="5-什么时候算训练完成"><a href="#5-什么时候算训练完成" class="headerlink" title="5. 什么时候算训练完成"></a>5. 什么时候算训练完成</h2><p><strong>通过训练集训练参数</strong>（随着训练轮数的增加，模型在训练集上的表现总是越来越好直至收敛的），训练<strong>指定轮数后在验证集里测效果</strong>。当模型<strong>在验证集准确率到达最大值时终止训练</strong>（从趋势上看，验证集准确率会逐步上升【期间欠拟合】，到达最高点后下降【此后过拟合】）</p>
<p><strong>“偏差-方差分解”（bias-variance decomposition）</strong>是解释学习算法泛化性能的一种重要工具。</p>
<p>泛化误差可分解为偏差、方差与噪声之和：</p>
<ul>
<li><strong>偏差</strong>：度量了学习算法的期望预测与真实结果的偏离程度，即刻画了<strong>学习算法本身的拟合能力</strong>；</li>
<li><strong>方差</strong>：度量了同样大小的训练集的变动所导致的学习性能的变化，即刻画了<strong>数据扰动所造成的影响</strong>；</li>
<li><strong>噪声</strong>：表达了在当前任务上任何学习算法所能够达到的期望泛化误差的下界，即刻画了<strong>学习问题本身的难度</strong>。</li>
</ul>
<p>偏差-方差分解说明，<strong>泛化性能</strong>是由<strong>学习算法的能力</strong>、<strong>数据的充分性</strong>以及<strong>学习任务本身的难度</strong>所共同决定的。给定学习任务，为了取得好的泛化性能，则需要使偏差较小，即能够充分拟合数据，并且使方差较小，即使得数据扰动产生的影响小。</p>
<p>在<strong>欠拟合（underfitting）</strong>的情况下，出现<strong>高偏差（high bias）</strong>的情况，即不能很好地对数据进行分类。</p>
<p>当模型设置的太复杂时，训练集中的一些噪声没有被排除，使得模型出现<strong>过拟合（overfitting）</strong>的情况，在验证集上出现<strong>高方差（high variance）</strong>的现象。</p>
<p>‼️当训练出一个模型以后，如果：</p>
<table>
<thead>
<tr>
<th><strong>训练集错误率</strong></th>
<th><strong>验证集错误率</strong></th>
<th><strong>现象分析</strong></th>
<th><strong>可能的问题</strong></th>
<th><strong>解决办法</strong></th>
</tr>
</thead>
<tbody><tr>
<td>较小</td>
<td>较大</td>
<td>方差较大</td>
<td>可能出现了过拟合</td>
<td>增加数据、使用正则化（如L1、L2正则化、Dropout）、减少模型复杂度</td>
</tr>
<tr>
<td>较大</td>
<td>较大（且相当）</td>
<td>偏差较大</td>
<td>可能出现了欠拟合</td>
<td>增加模型复杂度（如增加参数或层数）、尝试不同模型架构、减小正则化力度</td>
</tr>
<tr>
<td>较大</td>
<td>远大于训练集</td>
<td>方差和偏差都较大</td>
<td>模型表现很差</td>
<td>重新设计模型、增加数据量、适当正则化或调整模型训练超参数</td>
</tr>
<tr>
<td>较小</td>
<td>较小（且相差小）</td>
<td>方差和偏差都较小，模型效果好</td>
<td>模型性能较好，适合应用场景</td>
<td>持续优化但避免过度调整，以保持模型的良好泛化能力</td>
</tr>
</tbody></table>
<h2 id="6-在学习过程中如何减少欠拟合或过拟合"><a href="#6-在学习过程中如何减少欠拟合或过拟合" class="headerlink" title="6. 在学习过程中如何减少欠拟合或过拟合"></a>6. 在学习过程中如何减少欠拟合或过拟合</h2><h3 id="欠拟合（Underfitting）"><a href="#欠拟合（Underfitting）" class="headerlink" title="欠拟合（Underfitting）"></a>欠拟合（Underfitting）</h3><h4 id="现象"><a href="#现象" class="headerlink" title="现象"></a><strong>现象</strong></h4><p>模型无法很好地学习训练数据，表现为训练集和测试集的误差都较高。通常是因为模型复杂度不足或训练不足导致</p>
<h4 id="解决方法"><a href="#解决方法" class="headerlink" title="解决方法"></a><strong>解决方法</strong></h4><ul>
<li><p><strong>增加模型复杂度</strong>：选择更复杂的模型（例如，增加深度神经网络的层数或神经元数量）；使用更强的算法（如从线性回归换成多项式回归）</p>
</li>
<li><p><strong>增加特征数量</strong>：提取更多相关的特征，或者进行特征组合</p>
</li>
<li><p><strong>降低正则化力度</strong>：减小正则化参数（如L1&#x2F;L2正则化的系数）</p>
</li>
</ul>
<h3 id="过拟合（Overfitting）"><a href="#过拟合（Overfitting）" class="headerlink" title="过拟合（Overfitting）"></a><strong>过拟合（Overfitting）</strong></h3><h4 id="现象-1"><a href="#现象-1" class="headerlink" title="现象"></a><strong>现象</strong></h4><p>模型对训练数据学习过度，对测试数据表现较差。通常是因为模型复杂度过高或训练数据不足导致。</p>
<h4 id="解决方法-1"><a href="#解决方法-1" class="headerlink" title="解决方法"></a><strong>解决方法</strong></h4><ul>
<li><strong>增加训练数据</strong>：通过数据增强（Data Augmentation）生成更多样本；从外部来源获取更多数据</li>
<li><strong>减少模型复杂度</strong>：减少模型的层数或神经元数量；使用简单的算法（如减少决策树的深度）</li>
<li><strong>正则化</strong>：增加L1或L2正则化（权重惩罚）；使用 Dropout 随机丢弃神经元</li>
<li><strong>交叉验证</strong>：使用交叉验证选择最合适的模型和超参数</li>
<li><strong>早停法（Early Stopping）</strong>：在验证集性能不再提升时停止训练</li>
<li><strong>降低特征数量</strong>：通过特征选择或降维（如PCA）降低特征数量</li>
</ul>
<h4 id="正则化（Regularization）"><a href="#正则化（Regularization）" class="headerlink" title="正则化（Regularization）"></a>正则化（Regularization）</h4><p>为了减少因模型过于复杂而导致的过拟合，那就需要减少模型的复杂程度。一种直观的想法是舍弃一些不重要的属性，但这往往很难达到，因为这依赖于人的经验，相当于引入了不可控因素。而比较好的方法是<strong>在cost function中加入对模型复杂度的考虑</strong>，当模型过于复杂（即参数过大时）给予较大惩罚</p>
<h5 id="线性回归"><a href="#线性回归" class="headerlink" title="线性回归"></a><strong>线性回归</strong></h5><img src="/../../images/C1_W3_LinearCostRegularized.png"  style="zoom:70%;" />

<img src="/../../images/C1_W3_LinearGradientRegularized.png"  style="zoom:70%;" />

<h5 id="Logistic回归"><a href="#Logistic回归" class="headerlink" title="Logistic回归"></a>Logistic回归</h5><img src="/../../images/C1_W3_LogisticCostRegularized.png"  style="zoom:70%;" />

<img src="/../../images/C1_W3_LogisticGradientRegularized.png"  style="zoom:70%;" />

<h2 id="0-数据划分（训练集-验证集-测试集）"><a href="#0-数据划分（训练集-验证集-测试集）" class="headerlink" title="0. 数据划分（训练集&#x2F;验证集&#x2F;测试集）"></a>0. 数据划分（训练集&#x2F;验证集&#x2F;测试集）</h2><p>应用深度学习是一个典型的迭代过程</p>
<p>对于一个需要解决的问题的样本数据，在建立模型的过程中，数据会被划分为以下几个部分：</p>
<ul>
<li>训练集（train set）：用训练集对算法或模型进行<strong>训练</strong>过程；</li>
<li>验证集（development set）：利用验证集（又称为简单交叉验证集，hold-out cross validation set）进行<strong>交叉验证</strong>，<strong>选择出最好的模型</strong>；</li>
<li>测试集（test set）：最后利用测试集对模型进行测试，<strong>获取模型运行的无偏估计</strong>（对学习方法进行评估）。</li>
</ul>
<p>在<strong>小数据量</strong>的时代，如 100、1000、10000 的数据量大小，可以将数据集按照以下比例进行划分：</p>
<ul>
<li>无验证集的情况：70% &#x2F; 30%；</li>
<li>有验证集的情况：60% &#x2F; 20% &#x2F; 20%；</li>
</ul>
<p>而在如今的<strong>大数据时代</strong>，对于一个问题，我们拥有的数据集的规模可能是百万级别的，所以验证集和测试集所占的比重会趋向于变得更小。</p>
<p>验证集的目的是为了验证不同的算法哪种更加有效，所以验证集只要足够大到能够验证大约 2-10 种算法哪种更好，而不需要使用 20% 的数据作为验证集。如百万数据中抽取 1 万的数据作为验证集就可以了。</p>
<p>测试集的主要目的是评估模型的效果，如在单个分类器中，往往在百万级别的数据中，我们选择其中 1000 条数据足以评估单个模型的效果。</p>
<ul>
<li>100 万数据量：98% &#x2F; 1% &#x2F; 1%；</li>
<li>超百万数据量：99.5% &#x2F; 0.25% &#x2F; 0.25%（或者99.5% &#x2F; 0.4% &#x2F; 0.1%）</li>
</ul>

  </div>
</article>

    <div class="blog-post-comments">
        <div id="disqus_thread">
            <noscript>Please enable JavaScript to view the comments.</noscript>
        </div>
    </div>



        
          <div id="footer-post-container">
  <div id="footer-post">

    <div id="nav-footer" style="display: none">
      <ul>
        
          <li><a href="/">Home</a></li>
        
          <li><a href="/about/">About</a></li>
        
          <li><a href="/archives/">Writing</a></li>
        
          <li><a href="/tags/">Tag</a></li>
        
          <li><a href="/search/">Search</a></li>
        
          <li><a target="_blank" rel="noopener" href="https://github.com/smartyue076">Projects</a></li>
        
      </ul>
    </div>

    
    
      <div id="toc-footer" style="display: none">
        <ol class="toc"><li class="toc-item toc-level-2"><a class="toc-link" href="#1-%E9%80%89%E6%8B%A9%E5%90%88%E9%80%82%E7%9A%84%E6%A8%A1%E5%9E%8B"><span class="toc-number">1.</span> <span class="toc-text">1. 选择合适的模型</span></a></li><li class="toc-item toc-level-2"><a class="toc-link" href="#2-%E5%A6%82%E4%BD%95%E5%88%A4%E5%AE%9A%E5%BD%93%E5%89%8D%E6%A8%A1%E5%9E%8B%E5%8F%8A%E5%8F%82%E6%95%B0%E6%98%AF%E7%AC%A6%E5%90%88%E7%8E%B0%E5%AE%9E%E8%A6%81%E6%B1%82%E7%9A%84%EF%BC%88Cost-Function%EF%BC%89"><span class="toc-number">2.</span> <span class="toc-text">2. 如何判定当前模型及参数是符合现实要求的（Cost Function）</span></a></li><li class="toc-item toc-level-2"><a class="toc-link" href="#3-%E6%95%B0%E6%8D%AE%E9%A2%84%E5%A4%84%E7%90%86"><span class="toc-number">3.</span> <span class="toc-text">3. 数据预处理</span></a><ol class="toc-child"><li class="toc-item toc-level-4"><a class="toc-link" href="#%E7%89%B9%E5%BE%81%E7%BC%A9%E6%94%BE%EF%BC%88Feature-Scaling%EF%BC%89"><span class="toc-number">3.0.1.</span> <span class="toc-text">特征缩放（Feature Scaling）</span></a></li></ol></li></ol></li><li class="toc-item toc-level-2"><a class="toc-link" href="#4-%E5%A6%82%E4%BD%95%E8%B0%83%E6%95%B4%E6%A8%A1%E5%9E%8B%E5%8F%82%E6%95%B0"><span class="toc-number">4.</span> <span class="toc-text">4. 如何调整模型参数</span></a><ol class="toc-child"><li class="toc-item toc-level-3"><a class="toc-link" href="#%E5%AD%A6%E4%B9%A0%E6%96%B9%E6%B3%95%EF%BC%88%E6%A2%AF%E5%BA%A6%E4%B8%8B%E9%99%8D%E5%AD%A6%E4%B9%A0%E6%B3%95%EF%BC%88Gradient-Descent%EF%BC%89%EF%BC%89"><span class="toc-number">4.1.</span> <span class="toc-text">学习方法（梯度下降学习法（Gradient Descent））</span></a><ol class="toc-child"><li class="toc-item toc-level-4"><a class="toc-link" href="#%E6%A6%82%E5%BF%B5"><span class="toc-number">4.1.1.</span> <span class="toc-text">概念</span></a></li><li class="toc-item toc-level-4"><a class="toc-link" href="#%E7%9B%B4%E8%A7%82%E8%A1%A8%E7%A4%BA"><span class="toc-number">4.1.2.</span> <span class="toc-text">直观表示</span></a></li><li class="toc-item toc-level-4"><a class="toc-link" href="#%E5%85%AC%E5%BC%8F%E6%8E%A8%E5%AF%BC"><span class="toc-number">4.1.3.</span> <span class="toc-text">公式推导</span></a></li><li class="toc-item toc-level-4"><a class="toc-link" href="#%E5%A6%82%E4%BD%95%E8%AE%BE%E7%BD%AE%E5%90%88%E9%80%82%E7%9A%84%E5%AD%A6%E4%B9%A0%E7%8E%87%EF%BC%88Learning-Rate%EF%BC%89"><span class="toc-number">4.1.4.</span> <span class="toc-text">如何设置合适的学习率（Learning Rate）</span></a></li></ol></li></ol></li><li class="toc-item toc-level-2"><a class="toc-link" href="#5-%E4%BB%80%E4%B9%88%E6%97%B6%E5%80%99%E7%AE%97%E8%AE%AD%E7%BB%83%E5%AE%8C%E6%88%90"><span class="toc-number">5.</span> <span class="toc-text">5. 什么时候算训练完成</span></a></li><li class="toc-item toc-level-2"><a class="toc-link" href="#6-%E5%9C%A8%E5%AD%A6%E4%B9%A0%E8%BF%87%E7%A8%8B%E4%B8%AD%E5%A6%82%E4%BD%95%E5%87%8F%E5%B0%91%E6%AC%A0%E6%8B%9F%E5%90%88%E6%88%96%E8%BF%87%E6%8B%9F%E5%90%88"><span class="toc-number">6.</span> <span class="toc-text">6. 在学习过程中如何减少欠拟合或过拟合</span></a><ol class="toc-child"><li class="toc-item toc-level-3"><a class="toc-link" href="#%E6%AC%A0%E6%8B%9F%E5%90%88%EF%BC%88Underfitting%EF%BC%89"><span class="toc-number">6.1.</span> <span class="toc-text">欠拟合（Underfitting）</span></a><ol class="toc-child"><li class="toc-item toc-level-4"><a class="toc-link" href="#%E7%8E%B0%E8%B1%A1"><span class="toc-number">6.1.1.</span> <span class="toc-text">现象</span></a></li><li class="toc-item toc-level-4"><a class="toc-link" href="#%E8%A7%A3%E5%86%B3%E6%96%B9%E6%B3%95"><span class="toc-number">6.1.2.</span> <span class="toc-text">解决方法</span></a></li></ol></li><li class="toc-item toc-level-3"><a class="toc-link" href="#%E8%BF%87%E6%8B%9F%E5%90%88%EF%BC%88Overfitting%EF%BC%89"><span class="toc-number">6.2.</span> <span class="toc-text">过拟合（Overfitting）</span></a><ol class="toc-child"><li class="toc-item toc-level-4"><a class="toc-link" href="#%E7%8E%B0%E8%B1%A1-1"><span class="toc-number">6.2.1.</span> <span class="toc-text">现象</span></a></li><li class="toc-item toc-level-4"><a class="toc-link" href="#%E8%A7%A3%E5%86%B3%E6%96%B9%E6%B3%95-1"><span class="toc-number">6.2.2.</span> <span class="toc-text">解决方法</span></a></li><li class="toc-item toc-level-4"><a class="toc-link" href="#%E6%AD%A3%E5%88%99%E5%8C%96%EF%BC%88Regularization%EF%BC%89"><span class="toc-number">6.2.3.</span> <span class="toc-text">正则化（Regularization）</span></a><ol class="toc-child"><li class="toc-item toc-level-5"><a class="toc-link" href="#%E7%BA%BF%E6%80%A7%E5%9B%9E%E5%BD%92"><span class="toc-number">6.2.3.1.</span> <span class="toc-text">线性回归</span></a></li><li class="toc-item toc-level-5"><a class="toc-link" href="#Logistic%E5%9B%9E%E5%BD%92"><span class="toc-number">6.2.3.2.</span> <span class="toc-text">Logistic回归</span></a></li></ol></li></ol></li></ol></li><li class="toc-item toc-level-2"><a class="toc-link" href="#0-%E6%95%B0%E6%8D%AE%E5%88%92%E5%88%86%EF%BC%88%E8%AE%AD%E7%BB%83%E9%9B%86-%E9%AA%8C%E8%AF%81%E9%9B%86-%E6%B5%8B%E8%AF%95%E9%9B%86%EF%BC%89"><span class="toc-number">7.</span> <span class="toc-text">0. 数据划分（训练集&#x2F;验证集&#x2F;测试集）</span></a></li></ol>
      </div>
    

    <div id="share-footer" style="display: none">
      <ul>
  <li><a class="icon" target="_blank" rel="noopener" href="http://www.facebook.com/sharer.php?u=https://smartyue076.github.io/2025/01/21/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0%E5%9F%BA%E7%A1%80/"><i class="fab fa-facebook fa-lg" aria-hidden="true"></i></a></li>
  <li><a class="icon" target="_blank" rel="noopener" href="https://twitter.com/share?url=https://smartyue076.github.io/2025/01/21/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0%E5%9F%BA%E7%A1%80/&text=机器学习基础（一）基础方法与概念（线性回归为例）"><i class="fab fa-twitter fa-lg" aria-hidden="true"></i></a></li>
  <li><a class="icon" target="_blank" rel="noopener" href="http://www.linkedin.com/shareArticle?url=https://smartyue076.github.io/2025/01/21/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0%E5%9F%BA%E7%A1%80/&title=机器学习基础（一）基础方法与概念（线性回归为例）"><i class="fab fa-linkedin fa-lg" aria-hidden="true"></i></a></li>
  <li><a class="icon" target="_blank" rel="noopener" href="https://pinterest.com/pin/create/bookmarklet/?url=https://smartyue076.github.io/2025/01/21/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0%E5%9F%BA%E7%A1%80/&is_video=false&description=机器学习基础（一）基础方法与概念（线性回归为例）"><i class="fab fa-pinterest fa-lg" aria-hidden="true"></i></a></li>
  <li><a class="icon" href="mailto:?subject=机器学习基础（一）基础方法与概念（线性回归为例）&body=Check out this article: https://smartyue076.github.io/2025/01/21/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0%E5%9F%BA%E7%A1%80/"><i class="fa-solid fa-envelope fa-lg" aria-hidden="true"></i></a></li>
  <li><a class="icon" target="_blank" rel="noopener" href="https://getpocket.com/save?url=https://smartyue076.github.io/2025/01/21/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0%E5%9F%BA%E7%A1%80/&title=机器学习基础（一）基础方法与概念（线性回归为例）"><i class="fab fa-get-pocket fa-lg" aria-hidden="true"></i></a></li>
  <li><a class="icon" target="_blank" rel="noopener" href="http://reddit.com/submit?url=https://smartyue076.github.io/2025/01/21/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0%E5%9F%BA%E7%A1%80/&title=机器学习基础（一）基础方法与概念（线性回归为例）"><i class="fab fa-reddit fa-lg" aria-hidden="true"></i></a></li>
  <li><a class="icon" target="_blank" rel="noopener" href="http://www.stumbleupon.com/submit?url=https://smartyue076.github.io/2025/01/21/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0%E5%9F%BA%E7%A1%80/&title=机器学习基础（一）基础方法与概念（线性回归为例）"><i class="fab fa-stumbleupon fa-lg" aria-hidden="true"></i></a></li>
  <li><a class="icon" target="_blank" rel="noopener" href="http://digg.com/submit?url=https://smartyue076.github.io/2025/01/21/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0%E5%9F%BA%E7%A1%80/&title=机器学习基础（一）基础方法与概念（线性回归为例）"><i class="fab fa-digg fa-lg" aria-hidden="true"></i></a></li>
  <li><a class="icon" target="_blank" rel="noopener" href="http://www.tumblr.com/share/link?url=https://smartyue076.github.io/2025/01/21/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0%E5%9F%BA%E7%A1%80/&name=机器学习基础（一）基础方法与概念（线性回归为例）&description="><i class="fab fa-tumblr fa-lg" aria-hidden="true"></i></a></li>
  <li><a class="icon" target="_blank" rel="noopener" href="https://news.ycombinator.com/submitlink?u=https://smartyue076.github.io/2025/01/21/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0%E5%9F%BA%E7%A1%80/&t=机器学习基础（一）基础方法与概念（线性回归为例）"><i class="fab fa-hacker-news fa-lg" aria-hidden="true"></i></a></li>
</ul>

    </div>

    <div id="actions-footer">
        <a id="menu" class="icon" href="#" onclick="$('#nav-footer').toggle();return false;"><i class="fa-solid fa-bars fa-lg" aria-hidden="true"></i> Menu</a>
        
          <a id="toc" class="icon" href="#" onclick="$('#toc-footer').toggle();return false;"><i class="fa-solid fa-list fa-lg" aria-hidden="true"></i> TOC</a>
        
        <a id="share" class="icon" href="#" onclick="$('#share-footer').toggle();return false;"><i class="fa-solid fa-share-alt fa-lg" aria-hidden="true"></i> Share</a>
        <a id="top" style="display:none" class="icon" href="#" onclick="$('html, body').animate({ scrollTop: 0 }, 'fast');"><i class="fa-solid fa-chevron-up fa-lg" aria-hidden="true"></i> Top</a>
    </div>

  </div>
</div>

        
        <footer id="footer">
  <div class="footer-left">
    Copyright &copy;
    
    
    2020-2025
    Martin XU
  </div>
  <div class="footer-right">
    <nav>
      <ul>
        <!--
       --><li><a href="/">Home</a></li><!--
     --><!--
       --><li><a href="/about/">About</a></li><!--
     --><!--
       --><li><a href="/archives/">Writing</a></li><!--
     --><!--
       --><li><a href="/tags/">Tag</a></li><!--
     --><!--
       --><li><a href="/search/">Search</a></li><!--
     --><!--
       --><li><a target="_blank" rel="noopener" href="https://github.com/smartyue076">Projects</a></li><!--
     -->
      </ul>
    </nav>
  </div>
</footer>

    </div>
    <!-- styles -->



  <link rel="preload" as="style" href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/6.4.0/css/all.min.css" crossorigin="anonymous" onload="this.onload=null;this.rel='stylesheet'"/>


    <!-- jquery -->

  <script src="https://cdnjs.cloudflare.com/ajax/libs/jquery/3.6.0/jquery.min.js" crossorigin="anonymous"></script>




<!-- clipboard -->

  
    <script src="https://cdnjs.cloudflare.com/ajax/libs/clipboard.js/2.0.7/clipboard.min.js" crossorigin="anonymous"></script>
  
  <script type="text/javascript">
  $(function() {
    // copy-btn HTML
    var btn = "<span class=\"btn-copy tooltipped tooltipped-sw\" aria-label=\"Copy to clipboard!\">";
    btn += '<i class="fa-regular fa-clone"></i>';
    btn += '</span>';
    // mount it!
    $(".highlight table").before(btn);
    var clip = new ClipboardJS('.btn-copy', {
      text: function(trigger) {
        return Array.from(trigger.nextElementSibling.querySelectorAll('.code')).reduce((str,it)=>str+it.innerText+'\n','')
      }
    });
    clip.on('success', function(e) {
      e.trigger.setAttribute('aria-label', "Copied!");
      e.clearSelection();
    })
  })
  </script>


<script src="/js/main.js"></script>

<!-- search -->

<!-- Baidu Analytics -->

  <script type="text/javascript">
        var _hmt = _hmt || [];
        (function() {
          var hm = document.createElement("script");
          hm.src = "https://hm.baidu.com/hm.js?2e6da3c375c8a87f5b664cea6d4cb29c";
          var s = document.getElementsByTagName("script")[0];
          s.parentNode.insertBefore(hm, s);
        })();
        </script>

<!-- Cloudflare Analytics -->

<!-- Disqus Comments -->

    <script type="text/javascript">
        var disqus_shortname = 'cactus-1';

        (function(){
            var dsq = document.createElement('script');
            dsq.type = 'text/javascript';
            dsq.async = true;
            dsq.src = '//' + disqus_shortname + '.disqus.com/embed.js';
            (document.getElementsByTagName('head')[0] || document.getElementsByTagName('body')[0]).appendChild(dsq);
        }());
    </script>

<!-- utterances Comments -->

</body>
</html>
